# vk_face_grabber
VK face grabber

## Отчет
### 1. Выгрузка датасета
1. Посмотрел описание vk API. Первой мыслью было использовать поиск пользователей по стране, но апи не дает больше 1000 результатов, можно, конечно, декомпозировать поиск по городам/полам/возрасту и таким образом получить больше пользователей, но это довольно долго, да и пользователей с неполными персональными данными выпадают.
2. Набросал простейший синхронный граббер с использование либы vk_api, посмотрел на производительность, разочаровался.
3. Решил переписать граббер асинхронно, поискал асинхронные библиотеки для работы с vk, но, посмотрев на них, понял, что две ручки проще самому написать.
4. Архитектура получившегося граббера вдохновлена работой с go.
5. Для выгрузки использовал наименьшие фотографии подходящие под критерий размера - меньше размер, меньше траффик, меньше времени на поиск лиц.


### 2. Время выгрузки
`python main.py  208,87s user 10,16s system 48% cpu 7:34,76 total`


На моей машинке выгрузка 256 профилей заняла ~7.5 минут, или ~1.75 секунд на профиль.
Таким образом, сборка 1e6 профилей займет ~20 дней.

### 3. Способы ускорения сбора данных
1. Запуск n воркеров параллельно позволит кратно увеличить производительность. Как я описал в комментарии в main.py, надо доделать следующее: использовать ProcessPoolExcecutor для параллельного запуска n процессов, каждый из которых асинхронно обрабатывает свой батч айдишников.
2. С использованием Celery построить пайплайн обработки и запускать в n воркеров, как например описано [тут](https://medium.com/@tonywangcn/how-to-build-docker-cluster-with-celery-and-rabbitmq-in-10-minutes-13fc74d21730) и [тут](https://medium.com/@tonywangcn/how-to-build-a-scaleable-crawler-to-crawl-million-pages-with-a-single-machine-in-just-2-hours-ab3e238d1c22).
3. Текущая архитектура достаточно просто переносится на микросервисы. Вместо каждого воркера поднимается сервис, общаются через RabbitMQ. сервисы запроса пользователей и фоток можно бы переписать на Go, их работа будет значительно быстрее.
4. Не очень нравится текущее решение с выдачей батча айдишников воркеру для обработки - лучше замкнуть их на единый генератор, который будет выдавать новый id для обработки при освобождении воркера.
5. Использованная модель для поиска лиц супер простая, в реальном коде будет, конечно что-то более  CPU-heavy, что будет сильно блокировать работу текущего решения. Поэтому вынос модели в отдельный сервис - приоритетная задача. Дополнительно стоит доработать функционал обработки фотографий батчами, если модель предоставляет такую возможность.

Собственно пункты 1,2 про одно и тоже, распараллеливание, на разных уровнях: либо в рамках одного сервиса, либо нескольких. Все они дают кратное увеличение производительности. Вариант 3, по сути, является кастомной версией варианта 2, и, за счет большей гибкости и низкоуровневых (относительно Celery) оптимизаций, даст несколько больший прирост.

Пункт 4 больше про возникающую сложность с тем, чтобы каждый из параллельных обработчиков обрабатывал свои уникальные id и не повторял уже проделанную работу.
При масштабировании граббера может возникнуть проблема с обработкой количества запросов на стороне vk, да и могут забанить ключ - есть смысл делать по ключу для каждого воркера и/или ограничивать с нашей стороны количество запросов.


### 4. Необходимые ресурсы для выгрузки 1e6 профилей в сутки
Текущее решение загружает ядро процессора на ~50% (возрастает в момент поиска кропов) и занимает ~100мб оперативной памяти, собирая при этом 1е6 профилей за 20 дней. Если мы хотим добиться сбора этого количества в сутки, то нужны:
1. GPU, на которой будет производиться поиск лиц. Позволит ускорить этот процесс и снять нагрузку с CPU.
2. Никаких серьезный вычислений на CPU не производится, поэтому количество ядер/потоков важнее частоты. Отлично подойдет какой-то Xeon/i9 или аналогичные на 8+ ядер.

Распараллеливание на 16+ воркеров + оптимизация нагрузки + вынесение поиска лиц на GPU как раз позволит получить прирост в ~20 раз для сбора 1е6 профилей в сутки.
### 5. Фильтрация от ложных кропов
1. Использование более качественной модели
2. Для фильтрации кропов нарисованных лиц, лиц вымышленных персонажей и так далее, кажется, можно будет использовать модель классификации "реальный/вымышленный" человек на фото.
3. Использование мета-информации о фотографиях - лайки, комменты, заполненность профиля.
